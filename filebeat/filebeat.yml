filebeat.prospectors:
  - type: log
    json.keys_under_root: true
    # Json key name, which value contains a sub JSON document produced by our application Console Appender
    json.message_key: log
    enabled: true
    encoding: utf-8
    document_type: docker
    paths:
      # Location of all our Docker log files (mapped volume in docker-compose.yml)
      - '/usr/share/filebeat/dockerlogs/data/*/*.log'
processors:
  # decode the log field (sub JSON document) if JSONencoded, then maps it's fields to elasticsearch fields
  - decode_json_fields:
      fields: ["log"]
      target: ""
      # overwrite existing target elasticsearch fields while decoding json fields
      overwrite_keys: true
  - add_docker_metadata: ~

filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
  reload.enabled: false

setup.template.settings:
  index.number_of_shards: 3


output.kafka:
  # initial brokers for reading cluster metadata
  hosts: ["kafka:9092"]
  # message topic selection + partitioning
  topic: log_stream
  partition.round_robin:
    reachable_only: false
  required_acks: 1
  compression: gzip
  max_message_bytes: 1000000

# Write Filebeat own logs only to file to avoid catching them with itself in docker log files
logging.to_files: true
logging.to_syslog: false

# X-pack optional module
#xpack.monitoring.enabled: true
#xpack.monitoring.elasticsearch: